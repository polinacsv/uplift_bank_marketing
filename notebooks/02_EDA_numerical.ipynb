{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Uplift Bank Marketing - Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set the correct data directory (absolute path)\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"../src/uplift_bank_marketing/data\"))\n",
    "os.makedirs(project_root, exist_ok=True)\n",
    "\n",
    "X_path = os.path.join(project_root, \"X_bank_marketing.csv\")\n",
    "y_path = os.path.join(project_root, \"y_bank_marketing.csv\")\n",
    "\n",
    "# Check if data already exists\n",
    "if os.path.exists(X_path) and os.path.exists(y_path):\n",
    "    print(\"Loading data from saved files...\")\n",
    "    X = pd.read_csv(X_path)\n",
    "    y = pd.read_csv(y_path)\n",
    "    print(\"Data loaded successfully.\")\n",
    "else:\n",
    "    print(\"Downloading data from UCI Machine Learning Repository...\")\n",
    "    bank_marketing = fetch_ucirepo(id=222)\n",
    "    \n",
    "    # Save features and target\n",
    "    X = bank_marketing.data.features\n",
    "    y = bank_marketing.data.targets\n",
    "\n",
    "    X.to_csv(X_path, index=False)\n",
    "    y.to_csv(y_path, index=False)\n",
    "\n",
    "    print(\"Data downloaded and saved successfully.\")\n",
    "\n",
    "# Combine features and target for analysis\n",
    "df = X.copy()\n",
    "df['target'] = y\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdays\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"campaign\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"previous\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pdays_contacted'] = (df['pdays'] != -1).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pdays_contacted'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the unique values of \"pdays_contacted\" and their counts in the DataFrame\n",
    "pdays_counts = df['pdays_contacted'].value_counts(dropna=False).reset_index()\n",
    "pdays_counts.columns = ['pdays_contacted_value', 'count']\n",
    "pdays_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the unique values of \"campaign\" and their counts in the DataFrame\n",
    "pdays_counts = df['campaign'].value_counts(dropna=False).reset_index()\n",
    "pdays_counts.columns = ['campaign_value', 'count']\n",
    "pdays_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['campaign_once'] = (df['campaign'] == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_result_full = pd.crosstab(\n",
    "    df[\"pdays_contacted\"], \n",
    "    df[\"campaign_once\"], \n",
    "    margins=True, \n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "crosstab_result_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab_result_full = pd.crosstab(\n",
    "    [df[\"pdays_contacted\"], \n",
    "    df[\"campaign_once\"]], \n",
    "    df[\"target\"],\n",
    "    margins=True, \n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "crosstab_result_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the unique values of \"previous\" and their counts in the DataFrame\n",
    "pdays_counts = df['previous'].value_counts(dropna=False).reset_index()\n",
    "pdays_counts.columns = ['previous_value', 'count']\n",
    "pdays_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the unique values of \"pdays\" and their counts in the DataFrame\n",
    "pdays_counts = df['pdays'].value_counts(dropna=False).reset_index()\n",
    "pdays_counts.columns = ['pdays_value', 'count']\n",
    "pdays_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdays\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"poutcome\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df\n",
    "\n",
    "# Crosstab including NaN as a column (converted to string \"NaN\")\n",
    "df_na['pdays_contacted_str'] = df_na['pdays_contacted'].astype(str)\n",
    "df_na['poutcome_str'] = df_na['poutcome'].astype(str)\n",
    "\n",
    "crosstab_result_full = pd.crosstab(\n",
    "    df_na[\"pdays_contacted_str\"], \n",
    "    df_na[\"poutcome_str\"], \n",
    "    margins=True, \n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "crosstab_result_full\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstab with NaN as a separate column\n",
    "crosstab_result_full = pd.crosstab(\n",
    "    [df_na[\"pdays_contacted_str\"], \n",
    "    df_na[\"poutcome_str\"]], \n",
    "    df_na[\"target\"], \n",
    "    margins=True, \n",
    "    dropna=False\n",
    ")\n",
    "\n",
    "crosstab_result_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstab of pdays vs poutcome\n",
    "crosstab_result = pd.crosstab(df[\"pdays_contacted\"], df[\"poutcome\"], dropna= False)\n",
    "crosstab_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying all numerical variables\n",
    "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Setting up the subplots (2 columns)\n",
    "num_cols = len(numerical_columns)\n",
    "fig, axes = plt.subplots(nrows=(num_cols + 1) // 2, ncols=2, figsize=(14, 6 * ((num_cols + 1) // 2)))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "# Flattening the axes for easy iteration\n",
    "axes = axes.flatten() if num_cols > 1 else [axes]\n",
    "\n",
    "# Plotting each numerical column in a subplot\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    sns.histplot(df[col], kde=True, color='darkorange', ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution of {col} (Numerical Variable)\")\n",
    "\n",
    "# Removing any unused subplots\n",
    "for i in range(len(axes)):\n",
    "    if i >= num_cols:\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_contact_column(data_in_function):\n",
    "    \"\"\"\n",
    "    Processes the 'contact' column:\n",
    "    - Renames it to 'cellular'.\n",
    "    - Encodes 'cellular' as 1 (cellular) and 0 (telephone).\n",
    "    - Creates a 'cellular_missing' column if any missing values exist.\n",
    "    \"\"\"\n",
    "    if \"contact\" in data_in_function.columns:\n",
    "        # Create a missing flag if any values are missing\n",
    "        if data_in_function['contact'].isnull().sum() > 0:\n",
    "            data_in_function['cellular_missing'] = data_in_function['contact'].isnull().astype(int)\n",
    "        \n",
    "        # Renaming the column\n",
    "        data_in_function.rename(columns={\"contact\": \"cellular\"}, inplace=True)\n",
    "        \n",
    "        # Encoding cellular as 1, telephone as 0\n",
    "        data_in_function[\"cellular\"] = data_in_function[\"cellular\"].str.lower().map(\n",
    "            {\"cellular\": 1, \"telephone\": 0}\n",
    "        ).fillna(0).astype(int)\n",
    "    \n",
    "    return data_in_function\n",
    "\n",
    "def handle_missing_values(data_in_function):\n",
    "    \"\"\"\n",
    "    Handles missing values in the DataFrame.\n",
    "    - For binary columns, creates a missing flag if needed.\n",
    "    - For multi-level categorical columns, replaces with \"unknown\".\n",
    "    \"\"\"\n",
    "    missing_cols = data_in_function.columns[data_in_function.isnull().sum() > 0]\n",
    "\n",
    "    for col in missing_cols:\n",
    "        if data_in_function[col].dtype == 'object':\n",
    "            # Binary Columns (Two Unique Values)\n",
    "            if data_in_function[col].nunique() <= 2:\n",
    "                if data_in_function[col].isnull().sum() > 0:\n",
    "                    missing_flag = f\"{col}_missing\"\n",
    "                    data_in_function[missing_flag] = data_in_function[col].isnull().astype(int)\n",
    "            else:\n",
    "                # Multi-Level Categorical Columns\n",
    "                data_in_function[col].fillna(\"unknown\", inplace=True)\n",
    "    \n",
    "    return data_in_function\n",
    "\n",
    "\n",
    "def encode_binary_columns(data_in_function):\n",
    "    \"\"\"\n",
    "    Encodes binary columns as 0/1 (no/yes).\n",
    "    - Applies to binary columns (two unique values).\n",
    "    \"\"\"\n",
    "    for col in data_in_function.select_dtypes(include=['object']).columns:\n",
    "        if data_in_function[col].nunique() <= 2:\n",
    "            data_in_function[col] = data_in_function[col].str.lower().map(\n",
    "                {\"yes\": 1, \"no\": 0, \"true\": 1, \"false\": 0, \"1\": 1, \"0\": 0}\n",
    "            )\n",
    "            data_in_function[col] = data_in_function[col].fillna(0).astype(int)  # Any unknown becomes 0\n",
    "    \n",
    "    return data_in_function\n",
    "\n",
    "\n",
    "def one_hot_encode_multilevel(data_in_function):\n",
    "    \"\"\"\n",
    "    Applies One-Hot Encoding to multi-level categorical columns.\n",
    "    - Retains \"unknown\" and drops the second most frequent category.\n",
    "    \"\"\"\n",
    "    categorical_cols = data_in_function.select_dtypes(include=['object']).columns\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        value_counts = data_in_function[col].value_counts()\n",
    "        values_to_drop = value_counts.index[1] if len(value_counts) > 1 else value_counts.index[0]\n",
    "        \n",
    "        if values_to_drop == \"unknown\":\n",
    "            values_to_drop = value_counts.index[2] if len(value_counts) > 2 else None\n",
    "        \n",
    "        # One-Hot Encoding (0/1)\n",
    "        one_hot = pd.get_dummies(data_in_function[col], prefix=col).astype(int)\n",
    "        data_in_function = pd.concat([data_in_function.drop(columns=[col]), one_hot], axis=1)\n",
    "        \n",
    "        # Drop the chosen column if it exists and is not \"unknown\"\n",
    "        drop_column = f\"{col}_{values_to_drop}\"\n",
    "        if values_to_drop and drop_column in data_in_function.columns and \"unknown\" not in drop_column:\n",
    "            data_in_function.drop(columns=[drop_column], inplace=True)\n",
    "    \n",
    "    return data_in_function\n",
    "\n",
    "\n",
    "def process_age_column(data_in_function):\n",
    "    \"\"\"\n",
    "    Processes the 'age' column:\n",
    "    - Bins the continuous 'age' column into age groups.\n",
    "    - One-Hot Encodes these age groups as 0/1, removing the reference category (18-25).\n",
    "    - Drops the original 'age' column.\n",
    "    \"\"\"\n",
    "    if \"age\" in data_in_function.columns:\n",
    "        # Define age bins and labels\n",
    "        bins = [18, 25, 35, 45, 55, 65, float(\"inf\")]\n",
    "        labels = [\"18-25\", \"26-35\", \"36-45\", \"46-55\", \"56-65\", \"66+\"]\n",
    "\n",
    "        # Bin the 'age' column\n",
    "        data_in_function[\"age_group\"] = pd.cut(data_in_function[\"age\"], bins=bins, labels=labels, right=False)\n",
    "\n",
    "        # One-Hot Encode the age groups (as 0/1, not boolean)\n",
    "        age_dummies = pd.get_dummies(data_in_function[\"age_group\"], prefix=\"age_group\", drop_first=True).astype(int)\n",
    "        data_in_function = pd.concat([data_in_function.drop(columns=[\"age\"]), age_dummies], axis=1)\n",
    "    \n",
    "    return data_in_function\n",
    "\n",
    "\n",
    "def process_balance_column(data_in_function):\n",
    "    \"\"\"\n",
    "    Processes the 'balance' column:\n",
    "    - Creates a 'balance_negative' column indicating if balance was negative (0/1).\n",
    "    - Applies log transformation to the absolute value of balance for normalization.\n",
    "    - Standardizes the transformed balance.\n",
    "    - Drops the original 'balance' column.\n",
    "    \"\"\"\n",
    "    if \"balance\" in data_in_function.columns:\n",
    "        # Create a negative balance indicator (0/1)\n",
    "        data_in_function['balance_negative'] = (data_in_function['balance'] < 0).astype(int)\n",
    "        \n",
    "        # Calculate the absolute balance and log-transform it\n",
    "        abs_balance = np.abs(data_in_function['balance'])\n",
    "        log_balance = np.log1p(abs_balance)  # log(1 + |balance|)\n",
    "        \n",
    "        # Standardize the transformed balance\n",
    "        scaler = StandardScaler()\n",
    "        data_in_function['balance_processed'] = scaler.fit_transform(log_balance.values.reshape(-1, 1))\n",
    "        \n",
    "        # Drop the original balance column\n",
    "        data_in_function.drop(columns=['balance'], inplace=True)\n",
    "    \n",
    "    return data_in_function\n",
    "\n",
    "\n",
    "def process_day_of_month_column(data_in_function):\n",
    "    \"\"\"\n",
    "    Processes the 'day_of_week' column (actually day of the month).\n",
    "    - Bins it into early, mid, and late periods.\n",
    "    - Creates binary columns for these periods with 0/1 values.\n",
    "    - Drops the original 'day_of_week' column.\n",
    "    - Drops the first category to avoid multicollinearity.\n",
    "    \"\"\"\n",
    "    if \"day_of_week\" in data_in_function.columns:\n",
    "        # Define custom bins: early, mid, late (1-31)\n",
    "        bins = [1, 10, 20, 31]\n",
    "        labels = [\"early_month\", \"mid_month\", \"late_month\"]\n",
    "        \n",
    "        # Bin the days into these categories\n",
    "        data_in_function['day_of_month_period'] = pd.cut(\n",
    "            data_in_function['day_of_week'], bins=bins, labels=labels, right=True\n",
    "        )\n",
    "\n",
    "        # One-Hot Encode the periods with 0/1 and drop first category\n",
    "        period_dummies = pd.get_dummies(\n",
    "            data_in_function['day_of_month_period'], \n",
    "            prefix=\"day_of_month\", \n",
    "            drop_first=True\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Combine and clean up\n",
    "        data_in_function = pd.concat([data_in_function.drop(columns=['day_of_week', 'day_of_month_period']), \n",
    "                                      period_dummies], axis=1)\n",
    "    \n",
    "    return data_in_function\n",
    "\n",
    "\n",
    "def process_treatment_columns(data_in_function):\n",
    "    \"\"\"\n",
    "    Processes the campaign-related columns:\n",
    "    - Transforms 'campaign' into binary treatment (0 = no contact, 1 = any contact).\n",
    "    - Drops 'campaign' (numerical exposure).\n",
    "    - Drops 'pdays' (redundant with 'previous').\n",
    "    - Converts 'previous' to binary (contacted before: yes/no).\n",
    "    - Drops 'duration' (data leakage).\n",
    "    \"\"\"\n",
    "    if \"campaign\" in data_in_function.columns:\n",
    "        # Binary treatment (0 = no contact, 1 = any contact)\n",
    "        data_in_function['treatment'] = (data_in_function['campaign'] > 0).astype(int)\n",
    "        \n",
    "        # Drop 'campaign' (numerical exposure)\n",
    "        data_in_function.drop(columns=['campaign'], inplace=True)\n",
    "    \n",
    "    if \"pdays\" in data_in_function.columns:\n",
    "        # Drop 'pdays' (highly correlated with 'previous')\n",
    "        data_in_function.drop(columns=['pdays'], inplace=True)\n",
    "    \n",
    "    if \"previous\" in data_in_function.columns:\n",
    "        # Binary indicator for prior contact (0 = no, 1 = yes)\n",
    "        data_in_function['previous_contacted'] = (data_in_function['previous'] > 0).astype(int)\n",
    "        # Drop 'previous' (numerical exposure)\n",
    "        data_in_function.drop(columns=['previous'], inplace=True)\n",
    "    \n",
    "    # Drop 'duration' (data leakage)\n",
    "    if \"duration\" in data_in_function.columns:\n",
    "        data_in_function.drop(columns=['duration'], inplace=True)\n",
    "    \n",
    "    return data_in_function\n",
    "\n",
    "\n",
    "def prepare_data(data_in_function):\n",
    "    \"\"\"\n",
    "    Complete data preparation function combining all steps.\n",
    "    - Processes 'contact' column (cellular).\n",
    "    - Handles missing values.\n",
    "    - Encodes binary columns.\n",
    "    - One-Hot Encodes multi-level categorical columns.\n",
    "    - Bins and encodes the 'age' column.\n",
    "    - Processes the 'balance' column (negative flag, log transform, standardize).\n",
    "    - Processes the 'day_of_week' (actually day of the month) column (binned with 0/1, drop first).\n",
    "    - Processes the campaign-related columns.\n",
    "    \"\"\"\n",
    "    data_in_function = process_contact_column(data_in_function)\n",
    "    data_in_function = handle_missing_values(data_in_function)\n",
    "    data_in_function = encode_binary_columns(data_in_function)\n",
    "    data_in_function = one_hot_encode_multilevel(data_in_function)\n",
    "    data_in_function = process_age_column(data_in_function)\n",
    "    data_in_function = process_balance_column(data_in_function)\n",
    "    data_in_function = process_day_of_month_column(data_in_function)\n",
    "    data_in_function = process_treatment_columns(data_in_function)\n",
    "    \n",
    "    return data_in_function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared = prepare_data(df)\n",
    "df_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying all numerical variables\n",
    "numerical_columns = df_prepared.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Setting up the subplots (2 columns)\n",
    "num_cols = len(numerical_columns)\n",
    "fig, axes = plt.subplots(nrows=(num_cols + 1) // 2, ncols=2, figsize=(14, 6 * ((num_cols + 1) // 2)))\n",
    "fig.tight_layout(pad=5.0)\n",
    "\n",
    "# Flattening the axes for easy iteration\n",
    "axes = axes.flatten() if num_cols > 1 else [axes]\n",
    "\n",
    "# Plotting each numerical column in a subplot\n",
    "for i, col in enumerate(numerical_columns):\n",
    "    sns.histplot(df_prepared[col], kde=True, color='darkorange', ax=axes[i])\n",
    "    axes[i].set_title(f\"Distribution of {col} (Numerical Variable)\")\n",
    "\n",
    "# Removing any unused subplots\n",
    "for i in range(len(axes)):\n",
    "    if i >= num_cols:\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
